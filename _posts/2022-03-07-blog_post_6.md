---
layout: post
title: Classifying Fake vs. Real News
---

Hello again and welcome back to your favorite blog! In today's tutorial we will be using tensorflow to classify news articles as fake or real news. Let's get started!


```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from nltk.corpus import stopwords

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

```

## Data Prep

First things first, let's go ahead and read in our training data and take a look at the dataset we have been given.


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df.head()
```





  <div id="df-07915542-014f-496e-9831-6fbd13867c29">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-07915542-014f-496e-9831-6fbd13867c29')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-07915542-014f-496e-9831-6fbd13867c29 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-07915542-014f-496e-9831-6fbd13867c29');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




Our next step is to remove the stopwords from the `title` and `text` columns. A stopword is a word that is usually considered to be uninformative. In the dataframe above, some examples include "for", "to", and "the".


```python
import nltk
nltk.download('stopwords')
stop = stopwords.words('english')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.


We will go ahead and write a function that will produce a `tf.data.dataset`. First, we will remove the stopwords from our pandas dataframe. Next we will specify the inputs and outputs for our `tf.data.dataset` so that `(title,text)` are the inputs and the `fake` column is the output. We will also batch our data to help increase the speed of training. 


```python
def make_dataset(df):
  '''
  A function that removes stopwords from a pandas dataframe and then converts
  the dataframe to a tf.data.dataset.

  df: the pandas dataframe
  '''
  # remove stopwords
  df['title_without_stopwords'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  # specify inputs and outputs of tf.data.dataset
  data = tf.data.Dataset.from_tensor_slices(
      (
          {
              'title' : df[['title_without_stopwords']], 
              'text' : df[['text_without_stopwords']]
          }, 
          {
              'fake' : df[['fake']]
          }
      )
  )
  data = data.batch(100)
  return data

```

Let's now call our function and construct our dataset.


```python
data = make_dataset(df)
```

As with any machine learning question, we will need to split our training data into validation and training data. This is done below, and 20% of our training data is set aside as validation data.


```python
# 20% of data will be used for validation
val_size = int(0.2*len(data))

# get validation and training data
validation_data = data.take(val_size)
training_data = data.skip(val_size)

# check size of training and validation data
len(training_data), len(validation_data)
```




    (180, 45)



### Base Rate

Let's take a look at the accuracy of a theoretical model before we apply any form of machine learning. This is called the base rate, and to determine it we will find the percentage of fake entries in a batch taken from the dataset.


```python
# get random batch
for input, output in training_data.take(1):
  fake = output['fake'].numpy()

# modify data from vertical to horizontal format
fake_arr = np.transpose(fake)[0]

# count total 1's (fake)
fake_total = fake_arr.sum()

# count all entries (fake and real)
total = len(fake_arr)

# compute base rate
base_rate = fake_total/total

print(base_rate)
```

    0.54


## Create Models

Time to write some models! First we need to format our input. Recall that we can have up to two input layers (this will depend on the model that we are using) that will consist of the titles and text from the articles. The `shape` of both the title and text columns is 1 because there is only one column corresponding to each respective input. The `name` parameter is simply so that we can remember it later, and the `dtype` parameter indicates the type of input it is.


```python
# inputs

title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

We will also add the following code to vectorize our layers and format them so that we can adapt them for our models.


```python
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer.adapt(training_data.map(lambda x, y: x['title']))
vectorize_layer.adapt(training_data.map(lambda x, y: x['text']))
```

### Model 1

Our first model uses only the title of the articles to predict whether an article is fake or not. We will pass our title_input through the following layers and then predict the output using the following code.


```python
#embedding
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

#output
main = layers.Dense(32, activation='relu')(title_features)
output = layers.Dense(2, name = "fake")(main)
```


```python
model1 = keras.Model(
    inputs = title_input,
    outputs = output
)
```


```python
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model1.fit(training_data, 
                    validation_data=validation_data,
                    epochs = 50, 
                    verbose = False)
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning:
    
    Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
    



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = 'accuracy')
plt.plot(history.history["val_accuracy"], label = 'validation accuracy')
plt.legend()

```





![image-example.png](/images/output_22_1.png)
    


Looks like we got our model ton be fairly accurate! Now let's perform the same process but this time we will only use the text of the articles in our model.

### Model 2


```python
#embedding
text_features = vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "embedding2")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

#output
main = layers.Dense(32, activation='relu')(text_features)
output = layers.Dense(2, name = "fake")(main)
```


```python
model2 = keras.Model(
    inputs = text_input,
    outputs = output
)
```


```python
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = model2.fit(training_data, 
                    validation_data=validation_data,
                    epochs = 50, 
                    verbose = False)
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)



```python
plt.plot(history.history["accuracy"], label = 'accuracy')
plt.plot(history.history["val_accuracy"], label = 'validation accuracy')
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f6ceda74e50>





![image-example.png](/images/output_28_1.png)

    


Once again, looks pretty good! For our last model, we will take into account both the titles and the text when we construct our model. We predict that this will be our most accurate model yet.

### Model 3


```python
# combine text and title
main = layers.concatenate([title_features, text_features], axis = 1)

# output
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)
```


```python
model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```


```python
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = model3.fit(training_data, 
                    validation_data=validation_data,
                    epochs = 50, 
                    verbose = False)
```


```python
plt.plot(history.history["accuracy"], label = 'accuracy')
plt.plot(history.history["val_accuracy"], label = 'validation accuracy')
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f73a7a98510>




![image-example.png](/images/output_34_1.png)

    


Finally it's time to test our models on our validation data! As can be seen, model 3 was the most accurate. This was to be expected, as model 3 had the most input variables.


```python
# test accuracy on validation data

model1_score = model1.evaluate(validation_data)
print("model 1 score: " + str(model1_score))

model2_score = model2.evaluate(validation_data)
print("model 2 score: " + str(model2_score))

model3_score = model3.evaluate(validation_data)
print("model 3 score: " + str(model3_score))
```

    45/45 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9744
    model 1 score: [0.0773870125412941, 0.9744444489479065]
    45/45 [==============================] - 0s 9ms/step - loss: 0.1140 - accuracy: 0.9780
    model 2 score: [0.11402314156293869, 0.9779999852180481]
    45/45 [==============================] - 0s 10ms/step - loss: 0.0409 - accuracy: 0.9933
    model 3 score: [0.04091298580169678, 0.9933333396911621]


### Model Evaluation

Finally, we will do one last check of model 3 (our best model) but this time we will evaluate it on separate testing data obtained from the internet!


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
df_test = pd.read_csv(train_url)
test_data = make_dataset(df_test)
```

99% is very impressive, and we are happy with this result!



```python
final_score = model3.evaluate(test_data)
print("model 3 score on testing data: " + str(final_score))
```

    225/225 [==============================] - 2s 11ms/step - loss: 0.0151 - accuracy: 0.9968
    model 3 score on testing data: [0.015145923011004925, 0.996792733669281]


### Embedding Visualization

Our final step is to visualize the results of our model and take a closer look at some of the words that were most associated with fake news articles.


```python
weights = model3.get_layer('embedding1').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()  
```


```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```

From the plot below we see that some examples of words from opposite ends of the spectrum include "for", "to", "and", "or" in contrast to "opposition", "south", and "trumps".


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1",
                 size_max=10,
                 size = list(np.ones(len(embedding_df))),
                 hover_name = "word",
                 title="Embedding Visualization"
)

fig.show()
```
![image-example.png](/images/plotly.png)


Thanks for reading and make sure to keep an eye out for next week's blog post!

~ Emma

